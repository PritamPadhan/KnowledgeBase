{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* Install required libraries"
      ],
      "metadata": {
        "id": "aNrU1YjCM0iN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MPwEJ-N1MtuF"
      },
      "outputs": [],
      "source": [
        "!pip install -q groq PyPDF2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Import reuired libraries"
      ],
      "metadata": {
        "id": "RYmQxnX2M_O6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import PyPDF2\n",
        "from google.colab import userdata\n",
        "from groq import Groq\n"
      ],
      "metadata": {
        "id": "QJsJrpXMM6Mo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ibGVa9lLQteq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Initialize Groq API Client\n",
        "# ============================\n",
        "def initialize_groq_client(api_key):\n",
        "    \"\"\"Initialize the Groq API client.\"\"\"\n",
        "    return Groq(api_key=api_key)\n",
        "\n",
        "\n",
        "# ============================\n",
        "# Extract Text from PDF\n",
        "# ============================\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from each page of a PDF.\"\"\"\n",
        "    text_pages = []\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        for page in pdf_reader.pages:\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                text_pages.append(text.strip())  # Remove extra spaces\n",
        "    return text_pages\n",
        "\n",
        "\n",
        "# ============================\n",
        "# Preprocess Text\n",
        "# ============================\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text by converting to lowercase and removing special characters.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespaces\n",
        "    text = re.sub(r'[^a-z0-9\\s\\.,;:?!-]', '', text)  # Keep alphanumeric and punctuation\n",
        "    return text\n",
        "\n",
        "\n",
        "# ============================\n",
        "# Extract Knowledge Graph from Text using Groq API\n",
        "# ============================\n",
        "def get_knowledge_graph(client, preprocessed_text, retries=3):\n",
        "    \"\"\"\n",
        "    Extract structured entities and relationships from text using Groq API.\n",
        "    Returns a JSON object containing extracted entities and relationships.\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"You are an AI assistant that extracts structured knowledge from text. \"\n",
        "                                   \"Your task is to extract entities and relationships from the given text, \"\n",
        "                                   \"and return them in a structured JSON format as follows:\\n\\n\"\n",
        "                                   \"{\\n\"\n",
        "                                   '  \"entities\": [\\n'\n",
        "                                   '    {\"name\": \"Entity1\", \"properties\": {\"property1\": \"value1\", \"property2\": \"value2\"}},\\n'\n",
        "                                   '    {\"name\": \"Entity2\", \"properties\": {\"propertyA\": \"valueA\"}}\\n'\n",
        "                                   '  ],\\n'\n",
        "                                   '  \"relationships\": [\\n'\n",
        "                                   '    {\"subject\": \"Entity1\", \"relationship\": \"relates_to\", \"object\": \"Entity2\"}\\n'\n",
        "                                   '  ]\\n'\n",
        "                                   '}\\n\\n'\n",
        "                                   \"Ensure that each subject and object are individual entities.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": f\"\"\"Extract entities and relationships from the following text:\n",
        "\n",
        "                        {preprocessed_text}\n",
        "\n",
        "                        Respond strictly with a valid JSON object and nothing elseâ€”no explanations, introductions, or extra text.\"\"\"\n",
        "                    }\n",
        "                ],\n",
        "                model=\"llama3-70b-8192\",\n",
        "                temperature=0.5,\n",
        "                top_p=1,\n",
        "                stop=None,\n",
        "                stream=False,\n",
        "            )\n",
        "\n",
        "            # Parse the response\n",
        "            extracted_json = response.choices[0].message.content.strip()\n",
        "            parsed_data = json.loads(extracted_json)\n",
        "\n",
        "            # Validate the structure\n",
        "            if \"entities\" in parsed_data and \"relationships\" in parsed_data:\n",
        "                return parsed_data\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Attempt {attempt+1}: Failed to parse JSON. Retrying...\")\n",
        "            time.sleep(2)  # Wait before retrying\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during API call: {e}\")\n",
        "            return None\n",
        "\n",
        "    print(\"Failed to extract structured data after retries.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# ============================\n",
        "# Process PDF and Extract Knowledge Graph\n",
        "# ============================\n",
        "def process_pdf_and_extract_kg(pdf_path, client):\n",
        "    \"\"\"Extracts structured knowledge from a PDF file.\"\"\"\n",
        "    text_pages = extract_text_from_pdf(pdf_path)\n",
        "    print(\"Text extraction completed.\")\n",
        "\n",
        "    knowledge_graph = []\n",
        "\n",
        "    for idx, page_text in enumerate(text_pages):\n",
        "        print(f\"Processing Page {idx + 1}/{len(text_pages)}...\")\n",
        "\n",
        "        # Preprocess the text\n",
        "        preprocessed_text = preprocess_text(page_text)\n",
        "\n",
        "        # Get knowledge graph data\n",
        "        kg_data = get_knowledge_graph(client, preprocessed_text)\n",
        "\n",
        "        if kg_data:\n",
        "            knowledge_graph.append(kg_data)\n",
        "            # print(f\"Extraction for Page {idx + 1} completed.\")\n",
        "        else:\n",
        "            print(f\"Skipping Page {idx + 1} due to parsing issues.\")\n",
        "\n",
        "    return knowledge_graph\n",
        "\n",
        "\n",
        "# ============================\n",
        "# Save Knowledge Graph to JSON File\n",
        "# ============================\n",
        "def save_knowledge_graph(knowledge_graph, output_file):\n",
        "    \"\"\"Save the extracted knowledge graph to a JSON file.\"\"\"\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump(knowledge_graph, f, indent=2)\n",
        "    print(f\"Knowledge graph extraction completed. Results saved to {output_file}\")\n",
        "\n",
        "\n",
        "# ============================\n",
        "# Main Execution\n",
        "# ============================\n",
        "if __name__ == \"__main__\":\n",
        "    # Set your API key\n",
        "\n",
        "    API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "    # Set input and output file paths\n",
        "    PDF_PATH = \"/content/drive/MyDrive/KnowledgeDatabase/R048r12e.pdf\"\n",
        "    OUTPUT_FILE = \"/content/drive/MyDrive/KnowledgeGraphResults/kgf1.json\"\n",
        "\n",
        "    # Initialize API client\n",
        "    client = initialize_groq_client(API_KEY)\n",
        "\n",
        "    # Process the PDF and extract the knowledge graph\n",
        "    knowledge_graph = process_pdf_and_extract_kg(PDF_PATH, client)\n",
        "\n",
        "    # Save the extracted knowledge graph\n",
        "    save_knowledge_graph(knowledge_graph, OUTPUT_FILE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnAdw2CBQtb1",
        "outputId": "d5d6a046-39d9-4314-b14a-e15dcb86ec13"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text extraction completed.\n",
            "Processing Page 1/131...\n",
            "Processing Page 2/131...\n",
            "Failed to extract structured data after retries.\n",
            "Skipping Page 2 due to parsing issues.\n",
            "Processing Page 3/131...\n",
            "Processing Page 4/131...\n",
            "Processing Page 5/131...\n",
            "Processing Page 6/131...\n",
            "Processing Page 7/131...\n",
            "Processing Page 8/131...\n",
            "Processing Page 9/131...\n",
            "Processing Page 10/131...\n",
            "Processing Page 11/131...\n",
            "Processing Page 12/131...\n",
            "Processing Page 13/131...\n",
            "Processing Page 14/131...\n",
            "Processing Page 15/131...\n",
            "Processing Page 16/131...\n",
            "Processing Page 17/131...\n",
            "Processing Page 18/131...\n",
            "Processing Page 19/131...\n",
            "Processing Page 20/131...\n",
            "Processing Page 21/131...\n",
            "Processing Page 22/131...\n",
            "Processing Page 23/131...\n",
            "Processing Page 24/131...\n",
            "Processing Page 25/131...\n",
            "Processing Page 26/131...\n",
            "Processing Page 27/131...\n",
            "Processing Page 28/131...\n",
            "Processing Page 29/131...\n",
            "Processing Page 30/131...\n",
            "Processing Page 31/131...\n",
            "Processing Page 32/131...\n",
            "Processing Page 33/131...\n",
            "Processing Page 34/131...\n",
            "Processing Page 35/131...\n",
            "Processing Page 36/131...\n",
            "Processing Page 37/131...\n",
            "Processing Page 38/131...\n",
            "Processing Page 39/131...\n",
            "Processing Page 40/131...\n",
            "Processing Page 41/131...\n",
            "Processing Page 42/131...\n",
            "Processing Page 43/131...\n",
            "Processing Page 44/131...\n",
            "Processing Page 45/131...\n",
            "Processing Page 46/131...\n",
            "Processing Page 47/131...\n",
            "Processing Page 48/131...\n",
            "Processing Page 49/131...\n",
            "Processing Page 50/131...\n",
            "Processing Page 51/131...\n",
            "Processing Page 52/131...\n",
            "Processing Page 53/131...\n",
            "Processing Page 54/131...\n",
            "Processing Page 55/131...\n",
            "Processing Page 56/131...\n",
            "Processing Page 57/131...\n",
            "Processing Page 58/131...\n",
            "Processing Page 59/131...\n",
            "Processing Page 60/131...\n",
            "Processing Page 61/131...\n",
            "Processing Page 62/131...\n",
            "Processing Page 63/131...\n",
            "Processing Page 64/131...\n",
            "Processing Page 65/131...\n",
            "Processing Page 66/131...\n",
            "Processing Page 67/131...\n",
            "Processing Page 68/131...\n",
            "Processing Page 69/131...\n",
            "Processing Page 70/131...\n",
            "Processing Page 71/131...\n",
            "Processing Page 72/131...\n",
            "Processing Page 73/131...\n",
            "Processing Page 74/131...\n",
            "Processing Page 75/131...\n",
            "Processing Page 76/131...\n",
            "Processing Page 77/131...\n",
            "Processing Page 78/131...\n",
            "Processing Page 79/131...\n",
            "Processing Page 80/131...\n",
            "Processing Page 81/131...\n",
            "Processing Page 82/131...\n",
            "Processing Page 83/131...\n",
            "Processing Page 84/131...\n",
            "Processing Page 85/131...\n",
            "Processing Page 86/131...\n",
            "Processing Page 87/131...\n",
            "Processing Page 88/131...\n",
            "Processing Page 89/131...\n",
            "Processing Page 90/131...\n",
            "Processing Page 91/131...\n",
            "Processing Page 92/131...\n",
            "Processing Page 93/131...\n",
            "Processing Page 94/131...\n",
            "Processing Page 95/131...\n",
            "Processing Page 96/131...\n",
            "Processing Page 97/131...\n",
            "Processing Page 98/131...\n",
            "Processing Page 99/131...\n",
            "Processing Page 100/131...\n",
            "Processing Page 101/131...\n",
            "Processing Page 102/131...\n",
            "Processing Page 103/131...\n",
            "Processing Page 104/131...\n",
            "Processing Page 105/131...\n",
            "Processing Page 106/131...\n",
            "Processing Page 107/131...\n",
            "Processing Page 108/131...\n",
            "Processing Page 109/131...\n",
            "Processing Page 110/131...\n",
            "Processing Page 111/131...\n",
            "Processing Page 112/131...\n",
            "Processing Page 113/131...\n",
            "Processing Page 114/131...\n",
            "Processing Page 115/131...\n",
            "Processing Page 116/131...\n",
            "Processing Page 117/131...\n",
            "Processing Page 118/131...\n",
            "Processing Page 119/131...\n",
            "Processing Page 120/131...\n",
            "Processing Page 121/131...\n",
            "Processing Page 122/131...\n",
            "Processing Page 123/131...\n",
            "Processing Page 124/131...\n",
            "Processing Page 125/131...\n",
            "Processing Page 126/131...\n",
            "Processing Page 127/131...\n",
            "Processing Page 128/131...\n",
            "Processing Page 129/131...\n",
            "Processing Page 130/131...\n",
            "Processing Page 131/131...\n",
            "Knowledge graph extraction completed. Results saved to /content/drive/MyDrive/KnowledgeGraphResults/kgf1.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0WuiszMMTO4Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}